# Social Posts: Eval Harness

> Auto-generated by `/social`. Edit before posting.
> Source: patterns/testing/eval-harness/

---

## LinkedIn

**Character count: ~1,020/1,300**

GPT-4's code generation accuracy dropped from 52% to 10% across a single model update. Same inputs. Same prompts. No notification.

Without systematic evaluation, prompt changes ship on vibes. A developer tweaks summarization, tests three examples, ships it. Meanwhile, entity extraction broke and hallucination rates jumped from 3% to 18%. Nobody noticed for two weeks.

The fix isn't assertion tests â€” checking output contains a keyword or meets a length. Those catch structure, not semantics. What I'd want: continuous scorers with baseline comparison, sliced by category. That way "improved overall but degraded on entity extraction" gets caught before deploy.

The harness adds ~0.04ms/case overhead. Code-only scoring costs $0.88/day for a 50-case suite. One caught regression pays for months of that.

Full pattern with TypeScript + Python implementations, benchmarks, and runbook: https://github.com/kchia/production-llm-patterns/tree/main/patterns/testing/eval-harness

#LLM #ProductionML #SoftwareEngineering

---

## X (Twitter) Thread

### Tweet 1
**Character count: ~197/280**

1/ GPT-4's code gen accuracy dropped from 52% to 10% in a single model update. Same inputs, same prompts.

Without an eval harness, there's no way to know this happened until users complain. ðŸ§µ

### Tweet 2
**Character count: ~239/280**

2/ The naive approach: eyeball 5 examples, maybe write keyword assertions. This catches structure but not semantics â€” a response can pass every assertion and still be wrong.

Tag-based sliced scoring catches what flat assertions miss.

### Tweet 3
**Character count: ~241/280**

3/ Numbers from benchmarking:
- ~0.04ms/case harness overhead
- 100% concurrency efficiency
- Code-only scoring: $0.88/day for 50 cases
- GPT-4o-mini judges cost 17x less than GPT-4o

One caught regression pays for months of eval infra.

### Tweet 4
**Character count: ~181/280**

4/ Full eval harness pattern â€” dataset + scorers + comparison pipeline. TypeScript and Python, 6 benchmark scenarios, cost analysis, and ops runbook.

https://github.com/kchia/production-llm-patterns/tree/main/patterns/testing/eval-harness

---

## Source Material Used

- README.md â€” The Problem (Stanford/Berkeley GPT-4 accuracy drop, hallucination rate example), What I Would Not Do (eyeball vs assertion critique), Architecture (scorer-based comparison, tag slicing), Cost Analysis summary table
- benchmarks/results.md â€” ~0.04ms/case overhead, 100% concurrency efficiency
- cost-analysis.md â€” $0.88/day code-only scoring at 1K scale, 17x cost reduction with GPT-4o-mini judges

---

## Posting Notes

- [ ] Review and personalize the LinkedIn post (add your own reasoning and perspective)
- [ ] Verify all numbers match current benchmark/cost data
- [ ] Add the correct pattern URL before posting
- [ ] Post LinkedIn first, then X thread (LinkedIn rewards early engagement)
- [ ] Consider time zone â€” post during US business hours (9-11am ET) for best reach
